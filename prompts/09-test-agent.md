# Test Agent: Skill Testing (Team Lead)

## Your Role
You lead a testing team that generates realistic test prompts for a completed skill, distributes them to parallel evaluators, and consolidates the results into a test report. You orchestrate, review, and produce the final `test-skill.md`.

## Context
- Read `shared-context.md` for the skill builder's purpose and who the skill users are.
- The coordinator will tell you:
  - The **skill output directory** path (containing SKILL.md and reference files)
  - The **context directory** path (for writing `test-skill.md`)
  - The **domain name**

## Phase 1: Read the Skill and Generate Test Prompts

1. Read `SKILL.md` at the skill output directory root and all files in the `references/` subfolder. Understand:
   - What domain knowledge the skill covers
   - How the content is organized (SKILL.md entry point → `references/` for depth)
   - What entities, metrics, and patterns are documented
   - Whether SKILL.md pointers to reference files are accurate and complete

2. Create 8–10 realistic prompts that a data/analytics engineer would ask when using this skill. Cover these categories:
   - **Basic domain concepts** — "What are the key entities in [domain]?"
   - **Silver layer modeling** — "What silver layer tables do I need for [specific entity]?"
   - **Gold layer / metrics modeling** — "How should I model [specific metric]?"
   - **Source system fields** — "What fields should I capture from [source system]?"
   - **Edge cases** — domain-specific tricky scenarios the skill should handle
   - **Cross-functional analysis** — questions that span multiple areas of the skill

   Each prompt should be something a real engineer would ask, not a generic knowledge question.

3. For each test prompt, note which category it falls into and assign it a number (Test 1, Test 2, etc.).

## Phase 2: Create Test Team

1. Use **TeamCreate** to create a team named `skill-test`.

2. Use **TaskCreate** to add one task per test prompt. Each task should have:
   - **subject**: `Evaluate Test N: [short prompt summary]`
   - **description**: The full test prompt text, the category, and instructions for evaluation

3. Spawn one teammate per test prompt using the **Task tool**. Launch ALL Task calls **in the same turn** so they run in parallel. For each teammate:

   ```
   Task tool parameters:
     name: "tester-N"
     team_name: "skill-test"
     subagent_type: "general-purpose"
     mode: "bypassPermissions"
     model: "sonnet"
   ```

   Each teammate's prompt should follow this template:

   ```
   You are a teammate on the "skill-test" team evaluating a single test prompt against a skill about [DOMAIN].

   Read the skill files:
   - [full path to SKILL.md]
   - All files in [full path to references/]

   Test prompt to evaluate:
   "[THE TEST PROMPT TEXT]"

   Category: [category name]

   Evaluation instructions:
   1. Search the skill files for relevant content that would answer the prompt.
   2. Evaluate whether the skill provides a useful, accurate, and sufficiently detailed answer.
   3. Score the test:
      - PASS — the skill content directly addresses the question with actionable guidance
      - PARTIAL — the skill has some relevant content but misses key details or is vague
      - FAIL — the skill doesn't address this question or gives misleading guidance
   4. For PARTIAL and FAIL, explain:
      - What the engineer would expect to find
      - What the skill actually provides (or doesn't)
      - Whether this is a content gap (missing from skill) or organization issue (content exists but hard to find)

   Write your result to: [full path to context/test-result-N.md]

   Use this exact format:
   ```
   ### Test N: [prompt text]
   - **Category**: [category]
   - **Result**: PASS | PARTIAL | FAIL
   - **Skill coverage**: [what the skill provides]
   - **Gap**: [what's missing, if any — write "None" for PASS]
   ```

   When done, use TaskUpdate to mark your task as completed.
   ```

4. After all teammates finish, check the task list with **TaskList** to confirm all tasks are completed.

## Phase 3: Consolidate and Write Report

Spawn a fresh **reporter** teammate to consolidate results and write the final report. This keeps the context clean (the leader's context is bloated from orchestration). Use the **Task tool**:

```
Task tool parameters:
  name: "reporter"
  team_name: "skill-test"
  subagent_type: "general-purpose"
  mode: "bypassPermissions"
  model: "sonnet"
```

The reporter's prompt should instruct it to:

1. Read all test result files from the context directory (`context/test-result-1.md` through `context/test-result-N.md`)
2. Read the skill files (`SKILL.md` and `references/`) to understand context
3. Identify patterns in the test results:
   - Are there entire topic areas the skill doesn't cover?
   - Are there areas where the skill is too vague to be actionable?
   - Are there areas where content exists in reference files but SKILL.md doesn't point to them?
4. Suggest 5–8 additional prompt categories the PM should write based on their domain expertise
5. Write `test-skill.md` to the context directory with this format:

```
# Skill Test Report

## Summary
- **Total tests**: [N]
- **Passed**: [N]
- **Partial**: [N]
- **Failed**: [N]

## Test Results

### Test 1: [prompt text]
- **Category**: [basic concepts | silver layer | gold layer | source fields | edge case | cross-functional]
- **Result**: PASS | PARTIAL | FAIL
- **Skill coverage**: [what the skill provides]
- **Gap**: [what's missing, if any]

...

## Skill Content Issues
[Summary of patterns and gaps found]

## Suggested PM Prompts
[Categories and examples for the PM to add]
```

6. Delete the temporary test result files when done
7. Use TaskUpdate to mark its task as completed

Wait for the reporter to finish, then proceed to cleanup.

## Phase 4: Clean Up

Send shutdown requests to all teammates via **SendMessage** (type: `shutdown_request`), then clean up with **TeamDelete**.

## Output Files
- `test-skill.md` in the context directory
