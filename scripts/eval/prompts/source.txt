Design a silver-layer model that extracts Salesforce Opportunity data via REST API and transforms it into an analytics-ready opportunities fact table. Starting from bronze raw API responses, build a silver model that flattens nested JSON (OpportunityContactRole, OpportunityLineItem), handles standard fields (Amount, Stage, CloseDate, Probability) and custom fields for PS/MS deals (Service_Type__c, Contract_Term__c, MRR__c), and joins to related dimension tables (accounts, contacts, products). Implement incremental extraction using SystemModstamp to capture only changed records, handle SOQL query limits (200 records per query, 2000 character query length), and manage field-level security that may hide sensitive fields from the API. How should you structure the model to handle late-arriving updates, implement surrogate keys for slowly changing dimensions, and ensure idempotency when the same opportunity is extracted multiple times?
---
Build a silver-layer model that discovers and extracts Salesforce custom objects relevant to analytics (Project__c, Resource_Allocation__c, Timesheet__c) and transforms them into normalized dimension tables. Use Salesforce Metadata API to discover custom objects and fields, map Salesforce field types to warehouse types (picklists → enums, lookups → foreign keys, formula fields → computed columns, multi-select picklists → array columns), and handle record types that segment data within objects. Implement logic to identify analytics-relevant objects (exclude operational objects like audit logs, system objects), handle namespace prefixes from managed packages, and extract field-level metadata (labels, help text, validation rules) for data dictionary documentation. How should you structure the model to handle schema evolution when new custom fields are added, implement data quality tests for custom field constraints, and optimize extraction to minimize API calls?
---
Design a silver-layer model that reconstructs Salesforce object relationships and implements proper join logic for analytics queries. Map standard relationships (Account → Contact, Account → Opportunity, Opportunity → OpportunityLineItem), custom lookup relationships (Opportunity → Project__c, Project__c → Resource_Allocation__c), and master-detail relationships with rollup summaries. Handle polymorphic relationships (Task.WhoId can reference Contact or Lead), self-referential relationships (Account.ParentId for account hierarchies), and junction objects for many-to-many relationships (Campaign → CampaignMember → Lead/Contact). Implement bridge tables for complex relationships, denormalize frequently-joined paths for query performance, and maintain referential integrity with data quality tests. How should you handle relationship queries that exceed SOQL's 5-level limit, represent relationship paths in the dimensional model, and optimize join strategies for common reporting patterns (opportunity pipeline by account hierarchy, campaign attribution across multiple touches)?
---
Implement a robust incremental extraction strategy for Salesforce using SystemModstamp and REST API that minimizes API calls and ensures data consistency. Design a watermark management system that tracks last successful sync timestamp per object (stored in a control table), handles clock skew between Salesforce and extraction system (buffer window of 5 minutes), and manages partial failures (checkpoint progress, retry failed batches). Implement strategies for detecting hard deletes (query IsDeleted flag, maintain archive tables for deleted records), handle bulk updates that modify thousands of records (batch extraction, parallel API calls), and backfill logic for historical data loads (full refresh vs incremental catch-up). How should you balance extraction frequency (real-time vs hourly vs daily) against API limits (daily API call quotas, concurrent request limits), handle timezone conversions for timestamp comparisons (Salesforce uses UTC, warehouse may use local time), and ensure exactly-once delivery semantics when extraction jobs fail mid-run?
---
Create a comprehensive data quality validation framework for silver-layer Salesforce models that catches analytics issues before they corrupt downstream gold models. Implement validation tests for referential integrity (lookup fields reference existing records in dimension tables, no orphaned OpportunityLineItems after Opportunity deletion), business rule enforcement (required fields are not null, Stage follows valid progression, CloseDate is not in the past for open opportunities), and data anomaly detection (sudden spike in null values, unexpected picklist values, duplicate records with same external ID). Include Salesforce-specific validations (18-character IDs vs 15-character, case-sensitivity in SOQL queries, record locking during concurrent updates) and field-level security gaps (fields visible in UI but not in API, profile-based field access). How should you handle validation failures (quarantine bad records in error tables, alert data team via Slack/email, block downstream gold model builds), track data quality metrics over time (SLA dashboard), and provide actionable error messages that reference Salesforce record URLs for investigation?
